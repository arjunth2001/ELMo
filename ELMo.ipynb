{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e6de6-9cc1-4409-88b7-92ec236f5a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import  Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.autograd import Variable\n",
    "config ={\n",
    "\"elmo\": {\n",
    "        \"activation\": \"relu\",\n",
    "        \"filters\": [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]],\n",
    "        \"n_highway\": 2, \n",
    "        \"word_dim\": 300,\n",
    "        \"char_dim\": 50,\n",
    "        \"max_char_token\": 50,\n",
    "        \"min_count\":5,\n",
    "        \"max_length\":256,\n",
    "        \"output_dim\":150,\n",
    "        \"units\":256,\n",
    "        \"n_layers\":2,\n",
    "    },\n",
    "\"batch_size\":64,\n",
    "\"epochs\":5,\n",
    "\"lr\":0.00001,\n",
    "}\n",
    "model_path=\"./elmo_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83a1c9-db3a-48b2-8643-f8f40ad251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/corpus.json\") as f:\n",
    "    corpus = json.load(f)\n",
    "    #corpus = corpus[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46972562-aaae-43ce-9f05-b24d8332031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('device: ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c35eb-d982-4c11-b35d-d49f4669f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word2id,ch2id):\n",
    "        self.word2id = word2id\n",
    "        self.ch2id = ch2id\n",
    "        self.id2word = {i: word for word, i in word2id.items()}\n",
    "        self.id2ch = {i: char for char, i in ch2id.items()}\n",
    "    @classmethod\n",
    "    def from_corpus(cls,corpus,min_count=5):\n",
    "        word_count = Counter()\n",
    "        for sentence in corpus:\n",
    "            word_count.update(sentence.lower().split())\n",
    "        word_count = list(word_count.items())\n",
    "        word_count.sort(key=lambda x: x[1], reverse=True)\n",
    "        for i, (word, count) in enumerate(word_count):\n",
    "            if count < min_count:\n",
    "                break\n",
    "        vocab = word_count[:i]\n",
    "        vocab = [v[0] for v in vocab]\n",
    "        word_lexicon = {}\n",
    "        for special_word in ['<oov>', '<pad>']:\n",
    "            if special_word not in word_lexicon:\n",
    "                word_lexicon[special_word] = len(word_lexicon)\n",
    "        for word in vocab:\n",
    "            if word not in word_lexicon:\n",
    "                word_lexicon[word] = len(word_lexicon)\n",
    "        char_lexicon = {}\n",
    "        for special_char in ['<oov>', '<pad>']:\n",
    "            if special_char not in char_lexicon:\n",
    "                char_lexicon[special_char] = len(char_lexicon)\n",
    "        for sentence in corpus:\n",
    "            for word in sentence.split():\n",
    "                for ch in word:\n",
    "                    if ch not in char_lexicon:\n",
    "                        char_lexicon[ch] = len(char_lexicon)\n",
    "        return cls(word_lexicon,char_lexicon)\n",
    "    @classmethod\n",
    "    def from_file(cls,path):\n",
    "        with open(f\"{path}/tokenizer.json\") as f:\n",
    "            d = json.load(f)\n",
    "        return cls(d[\"word2id\"],d[\"ch2id\"])\n",
    "    \n",
    "    def tokenize(self,text,max_length=512,max_char=50):\n",
    "        oov_id, pad_id = self.word2id.get(\"<oov>\"), self.word2id.get(\"<pad>\")\n",
    "        w = torch.LongTensor(max_length).fill_(pad_id)\n",
    "        words = text.lower().split()\n",
    "        for i, wi in enumerate(words[:max_length]):\n",
    "            w[i] = self.word2id.get(wi, oov_id)\n",
    "        oov_id, pad_id = self.ch2id.get(\"<oov>\"), self.ch2id.get(\"<pad>\")\n",
    "        c = torch.LongTensor(max_length,max_char).fill_(pad_id)\n",
    "        for i, wi in enumerate(words[:max_length]):\n",
    "            for j,wij in enumerate(wi[:max_char]):\n",
    "                c[i][j]=self.ch2id.get(wij, oov_id)\n",
    "        return w , c\n",
    "\n",
    "    def save(self,path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except:\n",
    "            pass\n",
    "        tok ={\n",
    "            \"word2id\":self.word2id,\n",
    "            \"ch2id\":self.ch2id\n",
    "        }\n",
    "        with open(f\"{path}/tokenizer.json\",\"w\") as f:\n",
    "            json.dump(tok,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f73f4-1bba-48d9-a6a4-e9dbdee8d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_corpus(corpus,config[\"elmo\"][\"min_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029d240-7e53-41d9-832e-a673df56871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMoDataSet(Dataset):\n",
    "    def __init__(self,corpus,tokenizer):\n",
    "        self.corpus=corpus\n",
    "        self.tokenizer=tokenizer\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.corpus[idx]\n",
    "        w,c = self.tokenizer.tokenize(text,max_length=config[\"elmo\"][\"max_length\"],max_char=config[\"elmo\"][\"max_char_token\"])\n",
    "        return w,c\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef56fc9-5112-4d6a-9e72-af7d8dcf1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ELMoDataSet(corpus,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7cd71f-d409-44a5-bf46-9e68ec34d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da431f-48db-4e52-915b-435a2678f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based upon https://gist.github.com/Redchards/65f1a6f758a1a5c5efb56f83933c3f6e\n",
    "# Original Paper https://arxiv.org/abs/1505.00387\n",
    "class HighWay(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=1,activation= nn.functional.relu):\n",
    "        super(HighWay, self).__init__()\n",
    "        self._input_dim = input_dim\n",
    "        self._layers = nn.ModuleList([nn.Linear(input_dim, input_dim * 2) for _ in range(num_layers)])\n",
    "        self._activation = activation\n",
    "        for layer in self._layers:\n",
    "            layer.bias[input_dim:].data.fill_(1)\n",
    "    def forward(self, inputs):\n",
    "        current_input = inputs\n",
    "        for layer in self._layers:\n",
    "            projected_input = layer(current_input)\n",
    "            linear_part = current_input\n",
    "            nonlinear_part = projected_input[:, (0 * self._input_dim):(1 * self._input_dim)]\n",
    "            gate = projected_input[:, (1 * self._input_dim):(2 * self._input_dim)]\n",
    "            nonlinear_part = self._activation(nonlinear_part)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            current_input = gate * linear_part + (1 - gate) * nonlinear_part\n",
    "        return current_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b018881-0047-43f8-91b3-ba734ddad026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo(nn.Module):\n",
    "    def __init__(self,tokenizer,config):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.config=config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_embedder = nn.Embedding(len(tokenizer.word2id),config[\"elmo\"][\"word_dim\"],padding_idx=tokenizer.word2id.get(\"<pad>\"))\n",
    "        self.char_embedder = nn.Embedding(len(tokenizer.ch2id),config[\"elmo\"][\"char_dim\"],padding_idx=tokenizer.ch2id.get(\"<pad>\"))\n",
    "        self.output_dim = config[\"elmo\"][\"output_dim\"]\n",
    "        activation = config[\"elmo\"][\"activation\"]\n",
    "        if activation==\"relu\":\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation==\"tanh\":\n",
    "            self.act=nn.Tanh()\n",
    "        self.emb_dim = config[\"elmo\"][\"word_dim\"]\n",
    "        self.convolutions = []\n",
    "        filters = config[\"elmo\"][\"filters\"]\n",
    "        char_dim = config[\"elmo\"][\"char_dim\"]\n",
    "        for i, (width, num) in enumerate(filters):\n",
    "            conv = nn.Conv1d(in_channels=char_dim,\n",
    "                             out_channels=num,\n",
    "                             kernel_size=width,\n",
    "                             bias=True\n",
    "                             )\n",
    "            self.convolutions.append(conv)\n",
    "        self.convolutions = nn.ModuleList(self.convolutions)\n",
    "        self.n_filters = sum(f[1] for f in filters)\n",
    "        self.n_highway = config[\"elmo\"][\"n_highway\"]\n",
    "        self.highways = HighWay(self.n_filters, self.n_highway, activation=self.act)\n",
    "        self.emb_dim += self.n_filters\n",
    "        self.projection = nn.Linear(self.emb_dim, self.output_dim, bias=True)\n",
    "        self.f=[nn.LSTM(input_size = config[\"elmo\"][\"output_dim\"], hidden_size = config[\"elmo\"][\"units\"], batch_first=True)]\n",
    "        self.b=[nn.LSTM(input_size = config[\"elmo\"][\"output_dim\"], hidden_size = config[\"elmo\"][\"units\"], batch_first=True)]\n",
    "        for _ in range(config[\"elmo\"][\"n_layers\"]-1):\n",
    "            self.f.append(nn.LSTM(input_size = config[\"elmo\"][\"units\"], hidden_size = config[\"elmo\"][\"units\"], batch_first=True))\n",
    "            self.b.append(nn.LSTM(input_size = config[\"elmo\"][\"units\"], hidden_size = config[\"elmo\"][\"units\"], batch_first=True))\n",
    "        self.f = nn.ModuleList(self.f)\n",
    "        self.b = nn.ModuleList(self.b)\n",
    "        self.fwl = nn.Linear(in_features=config[\"elmo\"][\"units\"], out_features=len(tokenizer.word2id))\n",
    "        self.bwl = nn.Linear(in_features=config[\"elmo\"][\"units\"], out_features=len(tokenizer.word2id))\n",
    "    def forward(self, word_inp, chars_inp):\n",
    "        embs = []\n",
    "        batch_size, seq_len = word_inp.size(0), word_inp.size(1)\n",
    "        word_emb = self.word_embedder(Variable(word_inp))\n",
    "        embs.append(word_emb)\n",
    "        chars_inp = chars_inp.view(batch_size * seq_len, -1)\n",
    "        char_emb = self.char_embedder(Variable(chars_inp))\n",
    "        char_emb = char_emb.transpose(1, 2)\n",
    "        convs = []\n",
    "        for i in range(len(self.convolutions)):\n",
    "            convolved = self.convolutions[i](char_emb)\n",
    "            convolved, _ = torch.max(convolved, dim=-1)\n",
    "            convolved = self.act(convolved)\n",
    "            convs.append(convolved)\n",
    "        char_emb = torch.cat(convs, dim=-1)\n",
    "        char_emb = self.highways(char_emb)\n",
    "        embs.append(char_emb.view(batch_size, -1, self.n_filters))\n",
    "        token_embedding = torch.cat(embs, dim=2)\n",
    "        embeddings = self.projection(token_embedding)\n",
    "        fs = [embeddings[:, :-1, :]]         \n",
    "        bs = [embeddings[:, 1:, :]]\n",
    "        for fl,bl in zip(self.f,self.b):\n",
    "            o_f,_ = fl(fs[-1])\n",
    "            fs.append(o_f)\n",
    "            o_b,_ = bl(torch.flip(bs[-1],dims=[1,]))\n",
    "            bs.append(torch.flip(o_b,dims=(1,)))\n",
    "        return fs,bs\n",
    "    def save_model(self,path):\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except:\n",
    "            pass\n",
    "        torch.save(self.state_dict(),f'{path}/model.pt')\n",
    "        with open(f\"{path}/config.json\",\"w\") as f:\n",
    "            json.dump(self.config,f,indent=4)\n",
    "        self.tokenizer.save(path)\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls,path):\n",
    "        with open(f\"{path}/config.json\") as f:\n",
    "            config = json.load(f)\n",
    "        tokenizer = Tokenizer.from_file(path)\n",
    "        model = cls(tokenizer,config)\n",
    "        model.load_state_dict(torch.load(f'{path}/model.pt'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863d6c5-5cc7-4133-a3d4-45478a1e0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ELMo(tokenizer,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d11c9-2489-40c9-b9c1-2de4aa9bf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501e56c-544b-43ae-a13c-5d7cdf3a1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(),lr = config[\"lr\"])\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    for batch in tqdm(data_loader):\n",
    "        w , c = batch\n",
    "        w = w.to(device)\n",
    "        c = c.to(device)\n",
    "        f, b = model(w,c)\n",
    "        f = model.fwl(f[-1])  \n",
    "        b = model.bwl(b[-1])\n",
    "        loss = (\n",
    "            cross_entropy(f.reshape(-1,len(tokenizer.word2id)),w[:,1:].reshape(-1)) +\n",
    "            cross_entropy(b.reshape(-1,len(tokenizer.word2id)),w[:,:-1].reshape(-1)))/2\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        model.zero_grad()\n",
    "    model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d41865-91bc-4a03-8db6-9dfaffaad0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
